import subprocess
from pathlib import Path

from faster_whisper import WhisperModel
from video_metadata import Dialogue


MODEL_PATH = r"D:\python\models\models\faster-whisper-medium"


def extract_audio(video_path: str) -> str:
    """
    ä½¿ç”¨ ffmpeg æå– wav éŸ³é¢‘
    """
    audio_path = Path("outputs") / (Path(video_path).stem + ".wav")
    audio_path.parent.mkdir(exist_ok=True)

    cmd = [
        "ffmpeg",
        "-y",
        "-i", video_path,
        "-ac", "1",          # å•å£°é“
        "-ar", "16000",      # é‡‡æ ·ç‡16k
        "-vn",               # ä¸è¦è§†é¢‘
        str(audio_path)
    ]

    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

    return str(audio_path)


def transcribe_video(video_path: str):
    """
    ç¨³å®šç‰ˆ ASRï¼š
    1. å…ˆæå–éŸ³é¢‘
    2. å†è¿›è¡Œè¯†åˆ«
    """

    print("ğŸ§ æå–éŸ³é¢‘...")
    audio_path = extract_audio(video_path)

    print("ğŸ§  åŠ è½½ Whisper æ¨¡å‹...")
    model = WhisperModel(
        MODEL_PATH,
        device="cuda",
        compute_type="int8"
    )

    print("ğŸ™ å¼€å§‹è¯­éŸ³è¯†åˆ«...")

    segments, _ = model.transcribe(
        audio_path,
        language="zh",
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=2000),
    )

    dialogues = []

    for i, seg in enumerate(segments):
        print(f"è¯†åˆ« {i+1}: {seg.start:.1f}s â†’ {seg.end:.1f}s")

        dlg = Dialogue(
            start_time=seg.start,
            end_time=seg.end,
            speaker="æœªçŸ¥",
            text=seg.text.strip()
        )

        dialogues.append(dlg)

    print(f"âœ… å…±è¯†åˆ« {len(dialogues)} æ¡å°è¯")

    return dialogues
